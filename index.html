<!--
camera-app.html
Single-file camera app focused on Android/Chrome selfie mode, full-window preview, detailed dev readouts, live face detection with landmarks,
and a dynamic HUD that mounts a 10px horizontal bar between the eyes plus a perpendicular "cylinder" that scales with depth.

Instructions:
1) Save as camera-app.html and serve via HTTPS or http://localhost (e.g. `python -m http.server`).
2) Open in Chrome on Android or Chrome on desktop. Allow camera permission. Use the front camera when prompted.

Notes:
- Uses face-api.js (tiny detector + tiny 68-landmarks). Models are loaded from a CDN on demand.
- The overlay mapping handles object-fit:cover so the HUD follows the face even when the video is scaled/cropped to fill the window.
- The Dev Box shows real-time track settings, capabilities, landmark vectors, and computed depth metrics.
-->

<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1" />
  <title>Camera — Selfie + Face HUD + Dev Readouts</title>
  <style>
    html,body{height:100%;margin:0;background:#000;color:#fff;font-family:Inter,system-ui,Arial}
    /* full-viewport camera stage */
    .stage{position:fixed;inset:0;display:flex;}
    .videoWrap{position:relative;flex:1;overflow:hidden;background:#000}
    video{position:absolute;left:50%;top:50%;transform:translate(-50%,-50%);width:100%;height:100%;object-fit:cover;-webkit-transform:translate(-50%,-50%);}
    canvas.overlay{position:absolute;left:0;top:0;width:100%;height:100%;pointer-events:none}

    /* controls + dev box */
    .devBox{position:fixed;right:12px;top:12px;width:360px;max-height:calc(100vh - 24px);background:rgba(8,10,14,0.85);backdrop-filter:blur(6px);border-radius:10px;padding:10px;overflow:auto;border:1px solid rgba(255,255,255,0.04)}
    .devBox h3{margin:4px 0 8px 0;font-size:15px}
    .row{display:flex;gap:8px;align-items:center;margin-bottom:8px}
    label{font-size:12px;color:#9aa4b2}
    select,input,button{font-size:13px;padding:8px;border-radius:8px;border:1px solid rgba(255,255,255,0.06);background:transparent;color:inherit}
    textarea.json{width:100%;height:240px;background:transparent;border:1px solid rgba(255,255,255,0.04);color:#dfe8f2;padding:8px;border-radius:8px;font-family:monospace;font-size:12px}
    .muted{color:#9aa4b2;font-size:12px}
    .bigBtn{width:100%;padding:10px;margin-top:8px}
    .status{font-size:13px;padding:6px;border-radius:6px;background:rgba(255,255,255,0.02);margin-bottom:8px}

    /* small badge */
    .badge{position:fixed;left:12px;top:12px;padding:6px 10px;background:rgba(0,0,0,0.5);border-radius:999px;font-size:13px;border:1px solid rgba(255,255,255,0.04)}

    /* responsive tweak */
    @media (max-width:640px){ .devBox{width:320px;right:8px;top:8px;padding:8px} }
  </style>
</head>
<body>
  <div class="stage">
    <div class="videoWrap" id="videoWrap">
      <video id="video" autoplay playsinline muted></video>
      <canvas id="overlay" class="overlay"></canvas>
    </div>
  </div>

  <div class="badge" id="badge">Status: idle</div>

  <div class="devBox" id="devBox">
    <h3>Dev Box — Live Readouts</h3>
    <div class="status" id="status">idle</div>

    <div class="row"><button id="startBtn">Start Camera</button><button id="stopBtn">Stop</button></div>
    <div class="row"><label>Front camera</label><input type="checkbox" id="frontToggle" checked /></div>
    <div class="row"><label>Face detection</label><button id="faceToggle">Start Face</button></div>
    <div class="row"><label>Mount HUD</label><button id="hudToggle">Enable HUD</button></div>

    <div style="margin-top:8px"><label>Detector input size</label>
      <select id="inputSize"><option>128</option><option selected>224</option><option>320</option><option>416</option></select></div>

    <div style="margin-top:8px"><label>Min score</label>
      <input id="minScore" type="range" min="0" max="1" step="0.01" value="0.5" /><span id="minScoreLabel" class="muted">0.50</span></div>

    <div style="margin-top:8px"><label>HUD scale base</label>
      <input id="hudScale" type="range" min="0.2" max="3" step="0.01" value="1" /><span id="hudScaleLabel" class="muted">1.00</span></div>

    <div style="margin-top:8px"><label>Dev JSON (real-time)</label>
      <textarea id="devJson" class="json" readonly></textarea></div>

    <div style="margin-top:8px"><button id="downloadSnapshot" class="bigBtn">Download Snapshot (PNG)</button></div>

    <div class="muted" style="margin-top:8px;">Note: Allow camera permission. Models load from CDN; first load may take a second.</div>
  </div>

  <!-- scripts -->
  <script>
  // Minimal, focused implementation per spec.

  const state = {
    stream: null,
    track: null,
    faceModelLoaded: false,
    detectingFaces: false,
    hudEnabled: false,
    baseInterEye: null,
    lastDetection: null,
    fps: 0
  };

  const video = document.getElementById('video');
  const overlay = document.getElementById('overlay');
  const overlayCtx = overlay.getContext('2d');
  const videoWrap = document.getElementById('videoWrap');

  // UI
  const badge = document.getElementById('badge');
  const status = document.getElementById('status');
  const devJson = document.getElementById('devJson');
  const startBtn = document.getElementById('startBtn');
  const stopBtn = document.getElementById('stopBtn');
  const faceToggle = document.getElementById('faceToggle');
  const hudToggle = document.getElementById('hudToggle');
  const frontToggle = document.getElementById('frontToggle');
  const inputSizeSel = document.getElementById('inputSize');
  const minScoreEl = document.getElementById('minScore');
  const minScoreLabel = document.getElementById('minScoreLabel');
  const hudScale = document.getElementById('hudScale');
  const hudScaleLabel = document.getElementById('hudScaleLabel');
  const downloadSnapshot = document.getElementById('downloadSnapshot');

  minScoreEl.addEventListener('input', ()=>{ minScoreLabel.textContent = Number(minScoreEl.value).toFixed(2); });
  hudScale.addEventListener('input', ()=>{ hudScaleLabel.textContent = Number(hudScale.value).toFixed(2); });

  // helpers for mapping when video uses object-fit:cover
  function calcCoverTransform(){
    const vw = video.videoWidth, vh = video.videoHeight;
    const cw = overlay.clientWidth, ch = overlay.clientHeight;
    if(!vw||!vh) return null;
    const scale = Math.max(cw / vw, ch / vh);
    const displayW = vw * scale, displayH = vh * scale;
    const offsetX = (displayW - cw) / 2;
    const offsetY = (displayH - ch) / 2;
    return {scale, offsetX, offsetY};
  }

  function videoToOverlayCoords(pt){
    // pt: {x,y} in video intrinsic pixels (videoWidth/videoHeight)
    const t = calcCoverTransform(); if(!t) return pt;
    return { x: Math.round(pt.x * t.scale - t.offsetX), y: Math.round(pt.y * t.scale - t.offsetY) };
  }

  // draw HUD elements (bar + cylinder)
  function drawHud(detection){
    if(!detection) return;
    const landmarks = detection.landmarks;
    // left and right eye centers
    const leftEye = averagePoint(landmarks.getLeftEye());
    const rightEye = averagePoint(landmarks.getRightEye());
    // center between eyes
    const center = { x: (leftEye.x + rightEye.x)/2, y: (leftEye.y + rightEye.y)/2 };
    // inter-eye distance (pixels) -> depth proxy
    const interEye = distance(leftEye, rightEye);
    if(!state.baseInterEye) state.baseInterEye = interEye; // baseline on first detection
    // scale factor relative to baseline
    const scaleFactor = (interEye / state.baseInterEye) * Number(hudScale.value);

    // map to overlay display coords
    const c = videoToOverlayCoords(center);
    const ieDisplay = interEye * calcCoverTransform().scale;

    // horizontal bar
    const barHeight = 10; // fixed 10px thickness as requested
    const baseBarWidth = 200; // base width in video pixels; scaled by factor
    const barWidth = Math.max(24, baseBarWidth * scaleFactor);

    overlayCtx.save();
    overlayCtx.translate(0,0);
    // horizontal bar (centered at c.x,c.y)
    overlayCtx.fillStyle = 'rgba(255,80,80,0.95)';
    overlayCtx.fillRect(c.x - barWidth/2, c.y - barHeight/2, barWidth, barHeight);

    // draw small eye marker
    overlayCtx.fillStyle = 'rgba(255,255,255,0.8)';
    overlayCtx.beginPath(); overlayCtx.arc(c.x, c.y, 4, 0, Math.PI*2); overlayCtx.fill();

    // cylinder: perpendicular (vertical) line centered at c. We'll render a simple 2.5D cylinder
    const cylWidth = Math.max(8, 12 * scaleFactor);
    const cylLen = Math.max(30, 80 * scaleFactor);

    // draw back ellipse (darker)
    overlayCtx.fillStyle = 'rgba(60,120,220,0.45)';
    overlayCtx.beginPath(); overlayCtx.ellipse(c.x, c.y + cylLen/2, cylWidth, cylWidth/2, 0, 0, Math.PI*2); overlayCtx.fill();
    // draw body
    overlayCtx.fillStyle = 'rgba(80,160,255,0.6)';
    overlayCtx.fillRect(c.x - cylWidth/2, c.y - cylLen/2, cylWidth, cylLen);
    // draw front ellipse (lighter)
    overlayCtx.fillStyle = 'rgba(140,200,255,0.95)';
    overlayCtx.beginPath(); overlayCtx.ellipse(c.x, c.y - cylLen/2, cylWidth+2, (cylWidth+2)/2, 0, 0, Math.PI*2); overlayCtx.fill();

    // optional: small line showing inter-eye distance
    overlayCtx.strokeStyle = 'rgba(255,255,255,0.6)'; overlayCtx.lineWidth = 2;
    const le = videoToOverlayCoords(leftEye); const re = videoToOverlayCoords(rightEye);
    overlayCtx.beginPath(); overlayCtx.moveTo(le.x, le.y); overlayCtx.lineTo(re.x, re.y); overlayCtx.stroke();

    overlayCtx.restore();

    // store last detection data
    state.lastDetection = {center:c, interEye:ieDisplay, scaleFactor};
  }

  function averagePoint(points){
    const p = points.reduce((acc,pt)=>{ acc.x += pt.x; acc.y += pt.y; return acc; }, {x:0,y:0});
    return {x: p.x / points.length, y: p.y / points.length};
  }
  function distance(a,b){ const dx=a.x-b.x, dy=a.y-b.y; return Math.sqrt(dx*dx+dy*dy); }

  // drawing & clearing overlay
  function clearOverlay(){ overlayCtx.clearRect(0,0,overlay.width,overlay.height); }

  // load face-api and models
  let faceApiLoaded = false;
  async function ensureFaceModels(){ if(faceApiLoaded) return;
    try{
      await loadScript('https://unpkg.com/face-api.js@0.22.2/dist/face-api.min.js');
      const base = 'https://cdn.jsdelivr.net/gh/justadudewhohacks/face-api.js/models';
      // tiny detector + tiny landmark model
      await faceapi.nets.tinyFaceDetector.loadFromUri(base);
      await faceapi.nets.faceLandmark68TinyNet.loadFromUri(base);
      faceApiLoaded = true; state.faceModelLoaded = true; status.textContent = 'Face models loaded'; badge.textContent = 'Models loaded';
    }catch(e){ console.error('face model load failed',e); status.textContent = 'Model load failed'; badge.textContent = 'Model load failed'; throw e; }
  }

  // face detection loop
  let lastLoopTime = performance.now();
  async function faceLoop(){
    if(!state.detectingFaces || !state.stream) return;
    if(!faceApiLoaded) await ensureFaceModels();

    const now = performance.now();
    const dt = now - lastLoopTime; lastLoopTime = now; state.fps = 1000 / Math.max(dt,1);

    // detect single face with landmarks
    const inputSize = Number(inputSizeSel.value || 224);
    const options = new faceapi.TinyFaceDetectorOptions({ inputSize, scoreThreshold: Number(minScoreEl.value) });
    const result = await faceapi.detectSingleFace(video, options).withFaceLandmarks(true);

    clearOverlay();
    if(result){
      // draw landmarks for debugging
      const drawLandmarks = true;
      if(drawLandmarks){
        overlayCtx.fillStyle = 'rgba(0,255,120,0.9)';
        result.landmarks.positions.forEach(p=>{ const d = videoToOverlayCoords(p); overlayCtx.fillRect(d.x-1,d.y-1,3,3); });
      }

      if(state.hudEnabled) drawHud(result);
      status.textContent = 'Face detected — score ' + (result.score||0).toFixed(2);
      badge.textContent = `FPS: ${state.fps.toFixed(1)}  Faces:1`;
    } else {
      status.textContent = 'No face'; badge.textContent = `FPS: ${state.fps.toFixed(1)}  Faces:0`;
      state.lastDetection = null;
    }

    // update dev JSON
    const info = await gatherDevInfo(result);
    devJson.value = JSON.stringify(info, null, 2);

    requestAnimationFrame(faceLoop);
  }

  async function gatherDevInfo(result){
    const track = state.track;
    const settings = track && track.getSettings ? track.getSettings() : null;
    const caps = track && track.getCapabilities ? track.getCapabilities() : null;
    const constraints = track && track.getConstraints ? track.getConstraints() : null;

    return {
      time: new Date().toISOString(),
      streamActive: !!state.stream,
      settings, caps, constraints,
      lastDetection: state.lastDetection,
      detectionRaw: result ? {
        score: result.score,
        box: result.detection.box,
        landmarksCount: result.landmarks.positions.length,
      } : null
    };
  }

  // mapping overlay resolution to CSS size for crisp rendering
  function resizeOverlay(){
    const w = videoWrap.clientWidth, h = videoWrap.clientHeight;
    const ratio = window.devicePixelRatio || 1;
    overlay.style.width = w + 'px'; overlay.style.height = h + 'px';
    overlay.width = Math.round(w * ratio); overlay.height = Math.round(h * ratio);
    overlayCtx.setTransform(ratio,0,0,ratio,0,0); // scale drawing operations to CSS pixels
  }
  window.addEventListener('resize', resizeOverlay);

  // start/stop camera
  async function startCamera(){
    if(state.stream) stopCamera();
    const useFront = frontToggle.checked;
    const constraints = {
      audio: false,
      video: {
        facingMode: useFront ? 'user' : 'environment',
        width: { ideal: 1280 }, height: { ideal: 1280 }
      }
    };
    try{
      state.stream = await navigator.mediaDevices.getUserMedia(constraints);
      video.srcObject = state.stream; state.track = state.stream.getVideoTracks()[0];
      await video.play();
      resizeOverlay(); status.textContent = 'Camera running'; badge.textContent = 'Camera running';
    }catch(e){ console.error(e); status.textContent = 'Camera error: '+e.message; badge.textContent = 'Camera error'; }
  }

  function stopCamera(){ if(state.stream){ state.stream.getTracks().forEach(t=>t.stop()); state.stream = null; state.track = null; video.srcObject = null; status.textContent = 'Stopped'; badge.textContent = 'Stopped'; clearOverlay(); } }

  // toggles
  startBtn.addEventListener('click', startCamera);
  stopBtn.addEventListener('click', stopCamera);
  faceToggle.addEventListener('click', async ()=>{
    if(!state.detectingFaces){
      if(!faceApiLoaded) await ensureFaceModels();
      state.detectingFaces = true; faceToggle.textContent = 'Stop Face'; lastLoopTime = performance.now(); faceLoop();
    } else { state.detectingFaces = false; faceToggle.textContent = 'Start Face'; badge.textContent = 'Face stopped'; }
  });

  hudToggle.addEventListener('click', ()=>{ state.hudEnabled = !state.hudEnabled; hudToggle.textContent = state.hudEnabled ? 'Disable HUD' : 'Enable HUD'; });

  // snapshot
  downloadSnapshot.addEventListener('click', ()=>{
    // capture composed overlay + video frame
    const w = overlay.width, h = overlay.height; const ratio = window.devicePixelRatio || 1;
    const tmp = document.createElement('canvas'); tmp.width = w; tmp.height = h; const tctx = tmp.getContext('2d');
    // draw video frame scaled to overlay
    // draw using drawImage(video, sx,sy,sw,sh, dx,dy,dw,dh) mapping when object-fit:cover
    const vw = video.videoWidth, vh = video.videoHeight;
    const cw = overlay.clientWidth, ch = overlay.clientHeight;
    const scale = Math.max(cw / vw, ch / vh);
    const displayW = vw * scale, displayH = vh * scale;
    const offsetX = (displayW - cw) / 2; const offsetY = (displayH - ch) / 2;
    // source crop in video intrinsic pixels
    const sx = offsetX / scale; const sy = offsetY / scale; const sw = cw / scale; const sh = ch / scale;
    tctx.drawImage(video, sx, sy, sw, sh, 0, 0, w, h);
    // draw overlay on top
    tctx.drawImage(overlay, 0, 0, w, h);
    tmp.toBlob(b=>{ const url = URL.createObjectURL(b); const a = document.createElement('a'); a.href = url; a.download = 'snapshot.png'; a.click(); URL.revokeObjectURL(url); });
  });

  function loadScript(src){ return new Promise((res,rej)=>{ const s=document.createElement('script'); s.src=src; s.onload=res; s.onerror=rej; document.head.appendChild(s); }); }

  // auto-start camera if permission exists
  (async ()=>{ try{ await refreshDevices(); }catch(e){} })();

  async function refreshDevices(){ try{ const devs = await navigator.mediaDevices.enumerateDevices(); /* could populate selectors */ }catch(e){} }

  // keep dev JSON updated even when not detecting
  (function devLoop(){
    (async ()=>{
      const info = await gatherDevInfo(null);
      devJson.value = JSON.stringify(info, null, 2);
    })();
    setTimeout(devLoop, 500);
  })();

  async function gatherDevInfo(result){
    const t = state.track;
    const settings = t && t.getSettings ? t.getSettings() : null;
    const caps = t && t.getCapabilities ? t.getCapabilities() : null;
    const constraints = t && t.getConstraints ? t.getConstraints() : null;
    return {
      time: new Date().toISOString(),
      camera: { facing: frontToggle.checked ? 'user' : 'environment' },
      streamActive: !!state.stream,
      settings, caps, constraints,
      hudEnabled: state.hudEnabled,
      detection: state.lastDetection || null
    };
  }

  // convenience: start camera automatically for faster testing (comment out if undesired)
  // startCamera();

  </script>
</body>
</html>
