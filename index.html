<!--
camera-app.html
A single-file, GitHub Pages–ready camera web app that demonstrates many camera-related features
Designed for development, experimentation, and extension.

Goals included:
- enumerate and switch cameras
- start/stop stream with chosen resolution
- capture photo (canvas) + download
- record video (MediaRecorder) + download
- zoom control (via applyConstraints when supported)
- torch (flash) toggle when supported
- CSS & canvas filters, mirror mode, grid overlay
- basic QR/barcode scanning using jsQR (optional)
- optional face detection hook (face-api.js) with UI toggle
- graceful feature detection and fallbacks

Usage:
1) Save this file as camera-app.html in a new Git repository.
2) Open in a secure context (https or localhost). For local testing use a simple server (e.g., `python -m http.server`).
3) Push to GitHub and enable GitHub Pages (branch: main, folder: /) to host.

Notes & limitations:
- Browser APIs limit some functionality (true hardware flash, manual exposure controls, low-level autofocus) on many devices. This app attempts best-effort control via MediaStreamTrack.applyConstraints and ImageCapture when available.
- Face detection and barcode scanning rely on external libraries (loaded via CDN). Disable if you don't want them.

Author: Precision Software Engineer (Riley)
-->

<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Camera App — Feature Showcase</title>
  <style>
    :root{ --bg:#0b0f14; --panel:#0f1720; --muted:#9aa4b2; --accent:#06b6d4; }
    body{font-family:Inter, system-ui, -apple-system, Roboto, 'Helvetica Neue', Arial; margin:0; background:linear-gradient(180deg,#071021 0%, #081420 100%); color:#e6eef6}
    .app{display:grid;grid-template-columns:1fr 360px;gap:18px;padding:18px;height:100vh;box-sizing:border-box}
    .stage{background:linear-gradient(180deg,#081425 0%, #05202a 100%);border-radius:12px;padding:12px;display:flex;flex-direction:column;align-items:center;justify-content:center;position:relative}
    video, canvas{max-width:100%;border-radius:8px;background:#000}
    .controls{display:flex;flex-wrap:wrap;gap:8px;margin-top:12px}
    .panel{background:var(--panel);border-radius:12px;padding:12px;display:flex;flex-direction:column;gap:8px;height:100%;overflow:auto}
    label{font-size:12px;color:var(--muted)}
    select,input,button{padding:8px;border-radius:8px;border:1px solid rgba(255,255,255,0.06);background:transparent;color:inherit}
    button{cursor:pointer}
    .big{padding:12px 16px}
    .row{display:flex;gap:8px;align-items:center}

    /* overlays */
    .overlay-grid{position:absolute;inset:12px;pointer-events:none;display:none}
    .overlay-grid.show{display:block}
    .overlay-grid svg{width:100%;height:100%}

    /* small helper */
    .muted{color:var(--muted);font-size:13px}
    .status{font-size:13px;padding:8px;border-radius:8px;background:rgba(255,255,255,0.02)}
    .note{font-size:12px;color:var(--muted)}
  </style>
</head>
<body>
  <div class="app">
    <main class="stage">
      <!-- video container -->
      <div style="width:100%;max-width:980px;position:relative;">
        <video id="video" playsinline autoplay muted style="width:100%;height:auto; background:#000;" ></video>
        <canvas id="photoCanvas" style="display:none;position:absolute;left:0;top:0;width:100%;height:100%;"></canvas>

        <div class="overlay-grid" id="gridOverlay" aria-hidden="true">
          <svg viewBox="0 0 100 100" preserveAspectRatio="none">
            <g stroke="rgba(255,255,255,0.08)" stroke-width="0.6">
              <line x1="33" y1="0" x2="33" y2="100" />
              <line x1="66" y1="0" x2="66" y2="100" />
              <line x1="0" y1="33" x2="100" y2="33" />
              <line x1="0" y1="66" x2="100" y2="66" />
            </g>
          </svg>
        </div>
      </div>

      <div class="controls">
        <button id="startBtn" class="big">Start Camera</button>
        <button id="stopBtn" class="big">Stop</button>
        <button id="captureBtn" class="big">Capture Photo</button>
        <button id="recordBtn" class="big">Start Recording</button>
        <button id="downloadLastPhoto" class="big">Download Photo</button>
        <button id="downloadLastVideo" class="big">Download Video</button>
      </div>

      <div style="margin-top:8px;width:100%;display:flex;justify-content:space-between;align-items:center;">
        <div class="status" id="status">Idle</div>
        <div class="muted">This demo runs best in Chrome, Edge, or recent Firefox on desktop or Chrome on Android.</div>
      </div>
    </main>

    <aside class="panel">
      <div>
        <label>Camera device</label>
        <select id="deviceSelect"></select>
      </div>

      <div>
        <label>Resolution preset</label>
        <select id="resolutionSelect">
          <option value="default">Default</option>
          <option value="qvga">320x240 (QVGA)</option>
          <option value="vga">640x480 (VGA)</option>
          <option value="hd">1280x720 (HD)</option>
          <option value="fhd">1920x1080 (FHD)</option>
        </select>
      </div>

      <div class="row">
        <label>Zoom</label>
        <input id="zoomRange" type="range" min="1" max="3" step="0.01" value="1" style="flex:1" />
      </div>

      <div class="row">
        <label>Torch (if supported)</label>
        <button id="torchBtn">Toggle Torch</button>
      </div>

      <div class="row">
        <label>Mirror preview</label>
        <button id="mirrorBtn">Toggle Mirror</button>
      </div>

      <div>
        <label>Filter</label>
        <select id="filterSelect">
          <option value="none">None</option>
          <option value="grayscale(1)">Grayscale</option>
          <option value="sepia(1)">Sepia</option>
          <option value="contrast(1.2)">Contrast</option>
          <option value="saturate(1.5)">Saturate</option>
          <option value="blur(2px)">Blur</option>
        </select>
      </div>

      <div class="row">
        <label>Grid overlay</label>
        <button id="gridBtn">Toggle Grid</button>
      </div>

      <div>
        <label>Barcode / QR scan</label>
        <div class="row">
          <button id="scanToggle">Start Scanner</button>
          <div id="scanResult" class="muted">—</div>
        </div>
      </div>

      <div>
        <label>Face detection</label>
        <div class="row">
          <button id="faceToggle">Toggle Face Detection</button>
          <div id="faceStatus" class="muted">Off</div>
        </div>
      </div>

      <hr />
      <div class="note">Developer notes: This is a single-file starter. Extend with server-side uploads, AI models, and native wrappers (Capacitor / Tauri / Electron) for native APIs.</div>
    </aside>
  </div>

  <!-- External libraries (optional). They are loaded on demand to keep initial load light. -->
  <script>
  // --- Utility & state ---
  const state = {
    stream: null,
    track: null,
    devices: [],
    recorder: null,
    recordedChunks: [],
    lastPhotoBlob: null,
    lastVideoBlob: null,
    scanning: false,
    detectingFaces: false,
    faceModelLoaded: false
  };

  const video = document.getElementById('video');
  const photoCanvas = document.getElementById('photoCanvas');
  const deviceSelect = document.getElementById('deviceSelect');
  const resolutionSelect = document.getElementById('resolutionSelect');
  const startBtn = document.getElementById('startBtn');
  const stopBtn = document.getElementById('stopBtn');
  const captureBtn = document.getElementById('captureBtn');
  const recordBtn = document.getElementById('recordBtn');
  const downloadPhotoBtn = document.getElementById('downloadLastPhoto');
  const downloadVideoBtn = document.getElementById('downloadLastVideo');
  const statusEl = document.getElementById('status');
  const zoomRange = document.getElementById('zoomRange');
  const torchBtn = document.getElementById('torchBtn');
  const mirrorBtn = document.getElementById('mirrorBtn');
  const filterSelect = document.getElementById('filterSelect');
  const gridBtn = document.getElementById('gridBtn');
  const gridOverlay = document.getElementById('gridOverlay');
  const scanToggle = document.getElementById('scanToggle');
  const scanResult = document.getElementById('scanResult');
  const scanCanvas = document.createElement('canvas');
  const faceToggle = document.getElementById('faceToggle');
  const faceStatus = document.getElementById('faceStatus');

  // --- Device enumeration ---
  async function refreshDevices(){
    const devices = await navigator.mediaDevices.enumerateDevices();
    state.devices = devices.filter(d => d.kind === 'videoinput');
    deviceSelect.innerHTML = '';
    state.devices.forEach((d,i)=>{
      const opt = document.createElement('option'); opt.value = d.deviceId; opt.textContent = d.label || `Camera ${i+1}`; deviceSelect.appendChild(opt);
    });
  }

  // --- Start/Stop stream ---
  function resolutionConstraintPreset(preset){
    switch(preset){
      case 'qvga': return {width:320,height:240};
      case 'vga': return {width:640,height:480};
      case 'hd': return {width:1280,height:720};
      case 'fhd': return {width:1920,height:1080};
      default: return {}; }
  }

  async function startCamera(){
    if(state.stream){ stopCamera(); }
    const deviceId = deviceSelect.value || undefined;
    const preset = resolutionSelect.value;
    const constraints = { video: { deviceId: deviceId ? {exact: deviceId} : undefined, ...resolutionConstraintPreset(preset) }, audio: true };
    status('Starting camera...');
    try{
      state.stream = await navigator.mediaDevices.getUserMedia(constraints);
      video.srcObject = state.stream;
      state.track = state.stream.getVideoTracks()[0];
      status('Camera running — ' + (state.track.label || 'unknown device'));
      await refreshCapabilitiesUI();
    }catch(err){
      console.error(err); status('Error starting camera: ' + err.message);
    }
  }

  function stopCamera(){
    if(state.stream){ state.stream.getTracks().forEach(t => t.stop()); state.stream = null; state.track = null; video.srcObject = null; status('Stopped'); }
  }

  // --- Capture photo ---
  function capturePhoto(){
    if(!state.stream){ status('No camera'); return; }
    photoCanvas.width = video.videoWidth; photoCanvas.height = video.videoHeight;
    const ctx = photoCanvas.getContext('2d');
    // apply mirror if needed
    if(video.style.transform && video.style.transform.includes('scaleX(-1)')){
      ctx.translate(photoCanvas.width, 0); ctx.scale(-1,1);
    }
    // apply filter by drawing then applying CSS filters is tricky; we replicate video CSS filters on canvas via context.filter when available
    ctx.filter = window.getComputedStyle(video).getPropertyValue('filter') || 'none';
    ctx.drawImage(video, 0, 0, photoCanvas.width, photoCanvas.height);
    ctx.filter = 'none';
    photoCanvas.style.display = 'block';

    photoCanvas.toBlob(blob => { state.lastPhotoBlob = blob; status('Photo captured'); }, 'image/png');
  }

  // --- Video recording ---
  function startRecording(){
    if(!state.stream){ status('No stream to record'); return; }
    state.recordedChunks = [];
    const options = { mimeType: 'video/webm;codecs=vp9,opus' };
    try{
      state.recorder = new MediaRecorder(state.stream, options);
    }catch(e){
      state.recorder = new MediaRecorder(state.stream);
    }
    state.recorder.ondataavailable = e => { if(e.data && e.data.size) state.recordedChunks.push(e.data); };
    state.recorder.onstop = () => {
      state.lastVideoBlob = new Blob(state.recordedChunks, {type:'video/webm'});
      status('Recording stopped — ' + Math.round(state.lastVideoBlob.size/1024) + ' KB');
    };
    state.recorder.start();
    status('Recording...');
  }
  function stopRecording(){ if(state.recorder && state.recorder.state !== 'inactive') state.recorder.stop(); }

  // --- Downloads ---
  function downloadBlob(blob, filename){ if(!blob){ status('Nothing to download'); return; } const url = URL.createObjectURL(blob); const a = document.createElement('a'); a.href = url; a.download = filename; a.click(); URL.revokeObjectURL(url); }

  // --- Feature detection & advanced controls ---
  async function refreshCapabilitiesUI(){
    if(!state.track) return;
    const capabilities = state.track.getCapabilities ? state.track.getCapabilities() : {};
    // zoom
    if(capabilities.zoom){ zoomRange.min = capabilities.zoom.min; zoomRange.max = capabilities.zoom.max; zoomRange.step = capabilities.zoom.step || 0.01; zoomRange.value = capabilities.zoom.min; zoomRange.disabled = false; }
    else { zoomRange.disabled = true; }

    // torch support detection
    state.torchSupported = !!(capabilities.torch);
    torchBtn.disabled = !state.torchSupported;

    // load device list labels
    await refreshDevices();
  }

  async function setZoom(value){ if(!state.track) return; try{ await state.track.applyConstraints({ advanced: [{zoom: value}] }); status('Zoom set to ' + value); }catch(e){ console.warn(e); status('Zoom failed: ' + e.message); } }

  async function toggleTorch(){ if(!state.track) return; if(!state.torchSupported){ status('Torch not supported on this device'); return; } try{ const current = state.torchOn || false; await state.track.applyConstraints({ advanced: [{ torch: !current }] }); state.torchOn = !current; status('Torch ' + (state.torchOn ? 'on' : 'off')); }catch(e){ console.warn('torch error',e); status('Torch toggle failed: ' + e.message); } }

  // --- Mirror & filter ---
  function toggleMirror(){ if(video.style.transform && video.style.transform.includes('scaleX(-1)')){ video.style.transform = ''; } else { video.style.transform = 'scaleX(-1)'; } }
  function setFilter(val){ video.style.filter = val; }

  // --- Grid overlay ---
  function toggleGrid(){ gridOverlay.classList.toggle('show'); }

  // --- Scanner (QR / barcode using jsQR) ---
  let jsqrLoaded = false;
  async function ensureJsQR(){ if(jsqrLoaded) return; await loadScript('https://unpkg.com/jsqr/dist/jsQR.js'); jsqrLoaded = true; }
  async function scannerLoop(){
    if(!state.scanning) return;
    if(!state.stream) return;
    scanCanvas.width = video.videoWidth; scanCanvas.height = video.videoHeight;
    const ctx = scanCanvas.getContext('2d');
    ctx.drawImage(video,0,0,scanCanvas.width,scanCanvas.height);
    const imageData = ctx.getImageData(0,0,scanCanvas.width,scanCanvas.height);
    const code = jsQR(imageData.data, imageData.width, imageData.height);
    if(code){ scanResult.textContent = code.data; }
    else { scanResult.textContent = '—'; }
    requestAnimationFrame(scannerLoop);
  }

  async function toggleScanner(){
    if(!state.scanning){
      await ensureJsQR(); state.scanning = true; scanToggle.textContent = 'Stop Scanner'; status('Scanner running'); requestAnimationFrame(scannerLoop);
    } else { state.scanning = false; scanToggle.textContent = 'Start Scanner'; scanResult.textContent = '—'; status('Scanner stopped'); }
  }

  // --- Face detection (using face-api.js optional) ---
  async function ensureFaceAPI(){ if(state.faceModelLoaded) return;
    // load face-api and models (tiny face detector)
    await loadScript('https://unpkg.com/face-api.js@0.22.2/dist/face-api.min.js');
    // load models from official CDN - these are relatively small (tiny_face_detector)
    await faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/gh/justadudewhohacks/face-api.js/models');
    state.faceModelLoaded = true;
  }
  async function faceLoop(){
    if(!state.detectingFaces) return;
    if(!state.faceModelLoaded) await ensureFaceAPI();
    if(!state.stream) return;
    const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions());
    faceStatus.textContent = detections.length + ' face(s)';
    requestAnimationFrame(faceLoop);
  }
  async function toggleFaceDetection(){
    if(!state.detectingFaces){ state.detectingFaces = true; faceToggle.textContent = 'Stop Face Detection'; status('Face detection on'); faceLoop(); }
    else { state.detectingFaces = false; faceToggle.textContent = 'Start Face Detection'; faceStatus.textContent = 'Off'; status('Face detection off'); }
  }

  // --- Helpers ---
  function status(txt){ statusEl.textContent = txt; }
  function loadScript(src){ return new Promise((res,rej)=>{ const s = document.createElement('script'); s.src = src; s.onload = res; s.onerror = rej; document.head.appendChild(s); }); }

  // --- event wiring ---
  startBtn.addEventListener('click', startCamera);
  stopBtn.addEventListener('click', stopCamera);
  captureBtn.addEventListener('click', capturePhoto);
  recordBtn.addEventListener('click', ()=>{
    if(!state.recorder || state.recorder.state === 'inactive'){ startRecording(); recordBtn.textContent = 'Stop Recording'; }
    else { stopRecording(); recordBtn.textContent = 'Start Recording'; }
  });
  downloadPhotoBtn.addEventListener('click', ()=>downloadBlob(state.lastPhotoBlob, 'photo.png'));
  downloadVideoBtn.addEventListener('click', ()=>downloadBlob(state.lastVideoBlob, 'video.webm'));
  deviceSelect.addEventListener('change', ()=>{ startCamera(); });
  resolutionSelect.addEventListener('change', ()=>{ startCamera(); });
  zoomRange.addEventListener('input', ()=> setZoom(Number(zoomRange.value)));
  torchBtn.addEventListener('click', toggleTorch);
  mirrorBtn.addEventListener('click', ()=>{ toggleMirror(); });
  filterSelect.addEventListener('change', ()=> setFilter(filterSelect.value));
  gridBtn.addEventListener('click', toggleGrid);
  scanToggle.addEventListener('click', toggleScanner);
  faceToggle.addEventListener('click', toggleFaceDetection);

  // On load: enumerate devices and attempt to set a default camera label
  (async ()=>{ if(!navigator.mediaDevices) { status('Your browser does not support MediaDevices API.'); return; } await refreshDevices(); status('Ready — select a camera and start.'); })();

  // Clean up on page unload
  window.addEventListener('beforeunload', ()=>{ stopCamera(); if(state.recorder && state.recorder.state!=='inactive') state.recorder.stop(); });
  </script>
</body>
</html>
